# 21 December 2018

[Programming Languages - Lecture 1](https://www.youtube.com/watch?v=3N__tvmZrzc)

- There are lots of ways to classify programming languages:
  - High vs. low level
  - Functional versus Procedural versus Object-Oriented
  - Textual versus Graphical
  - Compiled versus Interpreted
  - etc.
- But many languages can't be classified neatly.
  - Many languages are multi-paradigm languages
- Science is often the work of classifying things
  - The attempt to classify programming languages is an attempt to make computing
    scientific.
  - Atomic theory:
    - Break something into smaller pieces.
    - Understand the rules for composing the smaller pieces.
  - Classification tries to use the similarity between different things as a way
    of understanding those things
  - Powerful idea:
    - understand the atoms
    - understand the rules of composition for those atoms
- The common classifications for programming languages fail because they are not
  based on understanding the atoms of the language and the rules for composing
  those atoms.
- Focus on the small language inside the large language.
  - Language features desugar to a smaller set of features.
  - "Inside everything large language is a small language"
  - To understand a programming language, try to find the small language inside of
    it.
  - Examples:
    - Logic
      - All logic gates desugar to the Nan gate
      - All logic gates desugar to the Nor gate
    - Arithmetic
      - All arithmetic operations desugar to looping addition with `-1` or `+1`.
    - All loop constructs desugar to a single loop construct
      - This could be a while loop
- Syntax sugar often desugars in more than one way.
  - Ultimately, there is more than one equivalent computation model:
    - Turing machine
    - Lambda calculus
- How you desugar is determined by what you want to optimize.
- The challenge is to prove that the desugared syntax is equivalent to the
  original.
- Recipe for understanding a programming language:
  - Desugar the language features to find smaller, essential language
- Recipe for designing a programming language:
  - Start with a small set of atomic features
  - _Don't_ start with the surface syntax, e.g. braces versus indented is
    not important.
  - Write an interpreter for that atomic language.
  - Play with the atoms. Try composing them.
  - Refine the core language.
  - Then add the syntax sugar.
  - You could even offer several syntaxes for the same core language
    (multiple interfaces).

**Reading List**

- ["Epigrams in Programming" by Alan Perlis](http://www.cs.yale.edu/homes/perlis-alan/quotes.html)

## Tokenizing with Megaparsec

The problem:

I'm working on a Lisp parser.
The only significance of whitespace is to delimit tokens.
What is the best way to write a parser in Haskell (Megaparsec) for this rule?

Combinators I know of:

- `sepBy` (Control.Monad.Combinators): parses 0 or more occurrences of a pattern,
  separated by one or more spaces.
    - The problem with this is that it doesn't work for things like lists.
    - `(` can be followed by a space
    - `)` can be preceded by a space
    - but in both cases, the space is optional
- `sepEndBy` (Control.Monad.Combinators): same as sepBy, except will also consume
  the separator if it follows the last occurrence of the pattern.
  This trailing occurrence is optional.
- What I need is an optional trailing occurrence and a required following
  occurrence.

Answer

I was overlooking the Text.Megaparsec.Char.Lexer module.
The parsers in that module are focused on tokenizing.
First you create a "space consumer" using `Text.Megaparsec.Char.Lexer.space`.
`space` takes three parsers: a whitespace parser, a line comment parser,
and a block comment parser.
This will then ignore anything matched by those parsers.
This is then given to `Text.Megaparsec.Char.Lexer.lexeme`.
`lexeme` takes the space consumer and a `Parser a` and will return `Parser a`
while ignoring anything that is consumed by the space consumer.
So `lexeme` is the generic tokenizer.
This is what I was looking for.

Something that tripped me up when using `lexeme`: it does apply the space consumer
at the start of the string.
It just consumes whitespace in the middle and at the end of the string.
The start of the string must match `Parser a`.
So you have to think of each token as:
`token := <pattern> | <pattern> <whitespace> | <pattern> <whitespace> <token>`.

## List parser stuck in infinite loop

I am writing a parser for a small subset of Lisp syntax.
The grammar looks like this.
(`+` means zero or more occurrences).

```
expression := list | atom | number | string

list := properList | improperList

properList := "(", <expression>+, ")"

improperList := "(", <expression>+, ".", <expression> ")"
```

I ran into a problem parsing lists.
Whenever I ran the expression parser, it would get stuck in an infinite loop.
The feedback I received indicates that this is because the production rule for a
sequence of expression is left recursive.
But I'm not sure where the left recursion is.

### What is left recursion?

[Parsec and Left Recursion](http://www.joelwilliamson.ca/programming/parsing/haskell/2015/02/04/Parsec-Left-Recursion.html)

A production rule is left-recursive when a symbol appears as a left-most symbol
in its own production rules:

```
foo := bar | foo , "(", bar ")" | foo , "[" , bar , "]"
```

Left-to-right parsers choke on left-recursive production rules.
If the parser fails to parse the string as `bar`, it will try the second 
production rule.
But the second rule starts with `foo`. 
So it goes back to the beginning, tries to parse as `bar`, fails, and moves on 
to the second rule, once again encountering `foo`, and recursing again.
This causes an infinite loop where nothing is consumed.


